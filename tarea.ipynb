{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b2c224-8321-409a-a385-fa9cbd1ba134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interfaz lista. Ejecuta las celdas y usa los widgets para interactuar.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff0bbf367dd4ddc809df6e92dbaf8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Modelo:', options=('Ollama (qwen3:8b)', 'OpenAI (gpt-4o-mi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "try:\n",
    "    from langchain.llms import Ollama\n",
    "    _HAS_OLLAMA = True\n",
    "except Exception:\n",
    "    _HAS_OLLAMA = False\n",
    "\n",
    "try:\n",
    "    from langchain import OpenAI\n",
    "    _HAS_OPENAI = True\n",
    "except Exception:\n",
    "    _HAS_OPENAI = False\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_MODEL = \"Ollama (qwen3:8b)\"\n",
    "MODEL_OPTIONS = [\n",
    "    \"Ollama (qwen3:8b)\",\n",
    "    \"OpenAI (gpt-4o-mini) - requires API key\",\n",
    "    \"Groq (placeholder)\",\n",
    "    \"Gemini (placeholder)\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_ollama_client(model_name: str = \"qwen3:8b\", temperature: float = 0.0, top_p: float = 1.0, top_k: int = None):\n",
    "   \n",
    "    if not _HAS_OLLAMA:\n",
    "        raise RuntimeError(\"LangChain no tiene el wrapper 'Ollama' disponible. Instala langchain>=0.0 y la dependencia correspondiente o usa el cliente 'ollama' nativo.\")\n",
    "\n",
    "    try:\n",
    "        client = Ollama(model=model_name, temperature=temperature)\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_llm_from_selection(selection: str, temperature: float, top_p: float, top_k: int):\n",
    "  \n",
    "    if selection.startswith(\"Ollama\"):\n",
    "        \n",
    "        model_token = \"qwen3:8b\"\n",
    "        return create_ollama_client(model_name=model_token, temperature=temperature, top_p=top_p, top_k=top_k)\n",
    "\n",
    "    if selection.startswith(\"OpenAI\"):\n",
    "        if not _HAS_OPENAI:\n",
    "            raise RuntimeError(\"LangChain OpenAI wrapper no disponible. Instala openai y langchain.\")\n",
    "        \n",
    "        return OpenAI(temperature=temperature)\n",
    "\n",
    "    \n",
    "    raise NotImplementedError(f\"La integración para {selection} no está implementada en este notebook. Agrega las credenciales y el wrapper correspondiente.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_local_file(file_upload_widget) -> List[Document]:\n",
    "    \n",
    "    docs = []\n",
    "    for filename, file_info in file_upload_widget.value.items():\n",
    "        content = file_info['content']\n",
    "        \n",
    "        tmp_path = f\"/tmp/{filename}\"\n",
    "        with open(tmp_path, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            try:\n",
    "                loader = PyPDFLoader(tmp_path)\n",
    "                docs.extend(loader.load())\n",
    "            except Exception:\n",
    "                # fallback to raw text read\n",
    "                with open(tmp_path, 'rb') as f:\n",
    "                    raw = f.read().decode(errors='ignore')\n",
    "                    docs.append(Document(page_content=raw, metadata={'source': filename}))\n",
    "        else:\n",
    "            with open(tmp_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                raw = f.read()\n",
    "                docs.append(Document(page_content=raw, metadata={'source': filename}))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def load_wikipedia(query: str, lang: str = 'es') -> List[Document]:\n",
    "   \n",
    "    try:\n",
    "        import wikipedia\n",
    "        wikipedia.set_lang(lang)\n",
    "        page = wikipedia.page(query)\n",
    "        text = page.content\n",
    "        return [Document(page_content=text, metadata={'source': f'wikipedia:{query}'})]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error cargando Wikipedia: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_documents(docs: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = []\n",
    "    for d in docs:\n",
    "        pieces = splitter.split_text(d.page_content)\n",
    "        for i, p in enumerate(pieces):\n",
    "            split_docs.append(Document(page_content=p, metadata={**d.metadata, 'chunk': i}))\n",
    "    return split_docs\n",
    "\n",
    "\n",
    "def filter_documents(docs: List[Document], keyword: str) -> List[Document]:\n",
    "    if not keyword:\n",
    "        return docs\n",
    "    filtered = [d for d in docs if keyword.lower() in d.page_content.lower()]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def summarize_documents_with_llm(docs: List[Document], llm, method: str = 'map_reduce') -> str:\n",
    "   \n",
    "    if not docs:\n",
    "        return \"(no hay documentos)\"\n",
    "    chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\")\n",
    "    return chain.run(docs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "system_template = PromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente experto que responde en español. Sigue las instrucciones del usuario.\n",
    "\"\"\")\n",
    "\n",
    "human_template = PromptTemplate.from_template(\"\"\"\n",
    "{user_input}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def build_messages(system_text: str, user_text: str) -> List:\n",
    "    return [SystemMessage(content=system_text), HumanMessage(content=user_text)]\n",
    "\n",
    "\n",
    "\n",
    "model_dropdown = widgets.Dropdown(options=MODEL_OPTIONS, value=DEFAULT_MODEL, description='Modelo:')\n",
    "temperature_slider = widgets.FloatSlider(value=0.7, min=0.0, max=1.0, step=0.01, description='Temp:')\n",
    "top_p_slider = widgets.FloatSlider(value=1.0, min=0.0, max=1.0, step=0.01, description='Top-p:')\n",
    "top_k_int = widgets.IntText(value=50, description='Top-k:')\n",
    "context_size = widgets.IntSlider(value=2048, min=256, max=65536, step=256, description='Context:')\n",
    "\n",
    "\n",
    "file_uploader = widgets.FileUpload(accept='*', multiple=True, description='Subir archivos')\n",
    "\n",
    "\n",
    "wiki_text = widgets.Text(value='', description='Wiki search:')\n",
    "wiki_button = widgets.Button(description='Cargar Wikipedia')\n",
    "\n",
    "\n",
    "chunk_size_widget = widgets.IntSlider(value=1000, min=200, max=5000, step=100, description='Chunk size:')\n",
    "chunk_overlap_widget = widgets.IntSlider(value=200, min=0, max=1000, step=50, description='Overlap:')\n",
    "filter_keyword = widgets.Text(value='', description='Filtrar por:')\n",
    "\n",
    "\n",
    "system_textarea = widgets.Textarea(value='Eres un asistente útil y conciso en español.', description='Sistema:')\n",
    "user_prompt = widgets.Textarea(value='Escribe tu pregunta aquí...', description='yo:')\n",
    "send_button = widgets.Button(description='Enviar al LLM')\n",
    "\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "_loaded_documents: List[Document] = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def on_wiki_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        q = wiki_text.value.strip()\n",
    "        if not q:\n",
    "            print(\"Ingresa un término para buscar en Wikipedia\")\n",
    "            return\n",
    "        try:\n",
    "            docs = load_wikipedia(q)\n",
    "            _loaded_documents.extend(docs)\n",
    "            print(f\"Cargado Wikipedia: {q} -> {len(docs)} documento(s)\")\n",
    "        except Exception as e:\n",
    "            print(\"Error cargando Wikipedia:\", e)\n",
    "\n",
    "\n",
    "wiki_button.on_click(on_wiki_button_clicked)\n",
    "\n",
    "\n",
    "def on_file_upload_change(change):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        if file_uploader.value:\n",
    "            try:\n",
    "                docs = load_local_file(file_uploader)\n",
    "                _loaded_documents.extend(docs)\n",
    "                print(f\"Cargados {len(docs)} documentos desde el uploader. Total en memoria: {len(_loaded_documents)}\")\n",
    "            except Exception as e:\n",
    "                print(\"Error leyendo archivos:\", e)\n",
    "\n",
    "\n",
    "file_uploader.observe(on_file_upload_change, names='value')\n",
    "\n",
    "\n",
    "def on_send_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(\"Preparando request...\")\n",
    "        # Preparar LLM\n",
    "        try:\n",
    "            llm = get_llm_from_selection(model_dropdown.value, temperature_slider.value, top_p_slider.value, top_k_int.value)\n",
    "        except NotImplementedError as nie:\n",
    "            print(nie)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(\"Error creando cliente LLM:\", e)\n",
    "            return\n",
    "\n",
    "        \n",
    "        docs = list(_loaded_documents)  \n",
    "        if docs:\n",
    "            docs = split_documents(docs, chunk_size=chunk_size_widget.value, chunk_overlap=chunk_overlap_widget.value)\n",
    "            docs = filter_documents(docs, filter_keyword.value)\n",
    "            print(f\"Documentos preparados: {len(docs)} chunks\")\n",
    "            # Resumir si son muchos\n",
    "            if len(docs) > 6:\n",
    "                try:\n",
    "                    print(\"Generando resumen de contexto...\")\n",
    "                    summary = summarize_documents_with_llm(docs[:10], llm)\n",
    "                    \n",
    "                    docs.insert(0, Document(page_content=summary, metadata={'source':'auto_summary'}))\n",
    "                except Exception as e:\n",
    "                    print(\"Resumen falló:\", e)\n",
    "\n",
    "       \n",
    "        sys_msg = system_textarea.value\n",
    "        human_msg = user_prompt.value\n",
    "        messages = build_messages(sys_msg, human_msg)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            prompt_full = f\"SYSTEM:\\n{sys_msg}\\n\\nUSER:\\n{human_msg}\\n\\nCONTEXT:\\n\"\n",
    "            for d in (docs or []):\n",
    "                prompt_full += f\"[source:{d.metadata.get('source','unknown')}] {d.page_content[:500]}\\n---\\n\"\n",
    "            print(\"Enviando prompt al LLM...\")\n",
    "            resp = llm(prompt_full)\n",
    "            print(\"--- RESPUESTA ---\")\n",
    "            print(resp)\n",
    "        except TypeError:\n",
    "           \n",
    "            try:\n",
    "                resp = llm(messages)\n",
    "                print(resp)\n",
    "            except Exception as e:\n",
    "                print(\"Error invocando LLM con messages:\", e)\n",
    "        except Exception as e:\n",
    "            print(\"Error llamando al LLM:\", e)\n",
    "\n",
    "\n",
    "send_button.on_click(on_send_button_clicked)\n",
    "\n",
    "\n",
    "left = widgets.VBox([\n",
    "    model_dropdown,\n",
    "    widgets.HBox([temperature_slider, top_p_slider, top_k_int]),\n",
    "    context_size,\n",
    "    widgets.Label(\"Cargar contexto:\"),\n",
    "    file_uploader,\n",
    "    widgets.HBox([wiki_text, wiki_button]),\n",
    "    widgets.Label(\"Transformaciones:\"),\n",
    "    chunk_size_widget,\n",
    "    chunk_overlap_widget,\n",
    "    filter_keyword,\n",
    "])\n",
    "\n",
    "right = widgets.VBox([\n",
    "    system_textarea,\n",
    "    user_prompt,\n",
    "    send_button,\n",
    "    output_area,\n",
    "])\n",
    "\n",
    "ui = widgets.HBox([left, right], layout=widgets.Layout(width='100%'))\n",
    "\n",
    "print(\"Interfaz lista. Ejecuta las celdas y usa los widgets para interactuar.\")\n",
    "display(ui)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208067d9-39e0-4d5e-b053-04b0efaef34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
