{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49af327-aa34-4648-824d-a5e251841fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace5980a353748af9ced21fc5ed44b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(VBox(children=(Dropdown(description='Modelo:', options=(('Ollama (local)', 'olla…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import asyncio\n",
    "from typing import List, Optional\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "try:\n",
    "    from langchain.schema import HumanMessage, SystemMessage\n",
    "    from langchain.chains import LLMChain\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.document_loaders import TextLoader\n",
    "\n",
    "    try:\n",
    "        from langchain.llms import Ollama\n",
    "    except Exception:\n",
    "        Ollama = None\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"**Error importando LangChain u otras dependencias:** {e} \\nAsegúrate de tener instalada una versión compatible de langchain.\"))\n",
    "\n",
    "try:\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "except Exception:\n",
    "    WikipediaLoader = None\n",
    "\n",
    "\n",
    "def load_uploaded_files(upload_widget: widgets.FileUpload) -> List[str]:\n",
    "    texts = []\n",
    "    for fname, fileinfo in upload_widget.value.items():\n",
    "        try:\n",
    "            content = fileinfo['content']\n",
    "           \n",
    "            txt = content.decode('utf-8', errors='ignore')\n",
    "        except Exception:\n",
    "           \n",
    "            try:\n",
    "                txt = fileinfo.get('content').decode('utf-8', errors='ignore')\n",
    "            except Exception:\n",
    "                txt = ''\n",
    "        texts.append(txt)\n",
    "    return texts\n",
    "\n",
    "\n",
    "async def call_llm_async(llm, messages: List):\n",
    "  \n",
    "    try:\n",
    "        if hasattr(llm, 'agenerate'):\n",
    "            res = await llm.agenerate(messages=[messages])\n",
    "            # result object varies\n",
    "            return res.generations[0][0].text\n",
    "        elif hasattr(llm, 'generate'):\n",
    "            res = llm.generate(messages)\n",
    "            # try to extract text\n",
    "            if hasattr(res, 'generations'):\n",
    "                return res.generations[0][0].text\n",
    "            return str(res)\n",
    "        else:\n",
    "            \n",
    "            prompt = ''.join([m.content if hasattr(m, 'content') else str(m) for m in messages])\n",
    "            return llm(prompt)\n",
    "    except Exception as e:\n",
    "        return f\"[Error llamando LLM: {e}]\"\n",
    "\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Ollama (local)', 'ollama'),\n",
    "        ('OpenAI (API)', 'openai'),\n",
    "        ('Gemini (API wrapper)', 'gemini'),\n",
    "        ('Groq (API wrapper)', 'groq'),\n",
    "        ('Otro/Custom', 'custom')\n",
    "    ],\n",
    "    value='ollama',\n",
    "    description='Modelo:'\n",
    ")\n",
    "\n",
    "temperature_slider = widgets.FloatSlider(value=0.2, min=0.0, max=1.0, step=0.01, description='Temperature:')\n",
    "top_p_slider = widgets.FloatSlider(value=0.9, min=0.0, max=1.0, step=0.01, description='Top-p:')\n",
    "top_k_slider = widgets.IntSlider(value=50, min=0, max=200, step=1, description='Top-k:')\n",
    "max_tokens_int = widgets.IntSlider(value=512, min=16, max=4096, step=16, description='Max tokens:')\n",
    "context_window = widgets.IntSlider(value=2048, min=256, max=32768, step=256, description='Context size:')\n",
    "\n",
    "param_box = widgets.VBox([model_dropdown, temperature_slider, top_p_slider, top_k_slider, max_tokens_int, context_window])\n",
    "\n",
    "file_uploader = widgets.FileUpload(accept='.txt,.md,.pdf,.docx', multiple=True)\n",
    "wiki_textbox = widgets.Text(value='LangChain', description='Wikipedia topic:')\n",
    "wiki_button = widgets.Button(description='Cargar Wikipedia')\n",
    "\n",
    "upload_box = widgets.VBox([widgets.Label('Subir archivos locales:'), file_uploader, widgets.HTML('<hr/>'), widgets.Label('Conectores en línea:'), wiki_textbox, wiki_button])\n",
    "\n",
    "\n",
    "split_size = widgets.IntSlider(value=2000, min=200, max=5000, step=100, description='Chunk size:')\n",
    "split_overlap = widgets.IntSlider(value=200, min=0, max=1000, step=50, description='Overlap:')\n",
    "filter_text = widgets.Text(value='', description='Filtrar (contiene):')\n",
    "summarize_button = widgets.Button(description='Generar resumen')\n",
    "transform_box = widgets.VBox([split_size, split_overlap, filter_text, summarize_button])\n",
    "\n",
    "\n",
    "system_prompt_area = widgets.Textarea(value='Eres un asistente útil, conciso y en español.', description='System:')\n",
    "human_input = widgets.Textarea(value='', description='Tu pregunta:')\n",
    "send_button = widgets.Button(description='Enviar al LLM')\n",
    "chat_output = widgets.Output()\n",
    "\n",
    "chat_box = widgets.VBox([system_prompt_area, human_input, send_button, chat_output])\n",
    "\n",
    "\n",
    "ui = widgets.HBox([widgets.VBox([param_box, upload_box, transform_box]), chat_box])\n",
    "display(ui)\n",
    "\n",
    "\n",
    "DOCUMENTS = []\n",
    "CHUNKS = []\n",
    "\n",
    "\n",
    "def load_wikipedia(topic: str):\n",
    "    if WikipediaLoader is None:\n",
    "        return [f\"[WikipediaLoader no disponible: instala una versión de langchain con loaders]\"]\n",
    "    try:\n",
    "        loader = WikipediaLoader(page_ids=[topic], load_max_docs=3)\n",
    "        docs = loader.load()\n",
    "        texts = [d.page_content for d in docs]\n",
    "        return texts\n",
    "    except Exception as e:\n",
    "        return [f\"[Error cargando Wikipedia: {e}]\"]\n",
    "\n",
    "\n",
    "def split_documents(texts: List[str], chunk_size: int, chunk_overlap: int):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = []\n",
    "    for t in texts:\n",
    "        docs.extend(splitter.split_text(t))\n",
    "    return docs\n",
    "\n",
    "\n",
    "async def build_llm_from_selection(provider_key: str, params: dict):\n",
    "    if provider_key == 'ollama':\n",
    "        if Ollama is None:\n",
    "            raise RuntimeError('Ollama no disponible (instala langchain con soporte Ollama o ollama-client).')\n",
    "\n",
    "        # Forzar uso del modelo local qwen3:8b\n",
    "        return Ollama(\n",
    "            model='qwen3:8b',\n",
    "            temperature=params.get('temperature', 0.2),\n",
    "            top_p=params.get('top_p', 0.9),\n",
    "            top_k=params.get('top_k', 50),\n",
    "            num_ctx=params.get('max_tokens', 512)\n",
    "        )\n",
    "\n",
    "    elif provider_key == 'openai':\n",
    "        if OpenAI is None:\n",
    "            raise RuntimeError('OpenAI wrapper no disponible (instala langchain y openai).')\n",
    "        key = os.getenv('OPENAI_API_KEY')\n",
    "        if not key:\n",
    "            display(Markdown('**Advertencia:** No hay clave de OpenAI configurada.'))\n",
    "        return OpenAI(\n",
    "            temperature=params.get('temperature', 0.2),\n",
    "            max_tokens=params.get('max_tokens', 512)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        if OpenAI is not None:\n",
    "            return OpenAI(\n",
    "                temperature=params.get('temperature', 0.2),\n",
    "                max_tokens=params.get('max_tokens', 512)\n",
    "            )\n",
    "        raise RuntimeError('Proveedor seleccionado no soportado en este entorno. Implementa tu wrapper.')\n",
    "\n",
    "\n",
    "\n",
    "async def on_wiki_button_clicked(b):\n",
    "    global DOCUMENTS, CHUNKS\n",
    "    topic = wiki_textbox.value.strip()\n",
    "    if not topic:\n",
    "        with chat_output:\n",
    "            print('Ingresa un tema para Wikipedia.')\n",
    "        return\n",
    "    with chat_output:\n",
    "        print(f'Cargando Wikipedia: {topic} ...')\n",
    "    texts = load_wikipedia(topic)\n",
    "    DOCUMENTS.extend(texts)\n",
    "    CHUNKS = split_documents(DOCUMENTS, chunk_size=split_size.value, chunk_overlap=split_overlap.value)\n",
    "    with chat_output:\n",
    "        print(f'Cargados {len(texts)} documentos. Chunks totales: {len(CHUNKS)}')\n",
    "\n",
    "\n",
    "async def on_summarize_clicked(b):\n",
    "    global CHUNKS\n",
    "    if len(CHUNKS) == 0:\n",
    "        with chat_output:\n",
    "            print('No hay chunks para resumir. Carga archivos o Wikipedia primero.')\n",
    "        return\n",
    "\n",
    "    sample = '\\n\\n'.join(CHUNKS[:5])\n",
    "\n",
    "    try:\n",
    "        params = {'temperature': temperature_slider.value, 'max_tokens': max_tokens_int.value}\n",
    "        llm_wrapper = await build_llm_from_selection(model_dropdown.value, params)\n",
    "        prompt = PromptTemplate(input_variables=['text'], template='Resumir brevemente en español el siguiente texto:\\n\\n{text}')\n",
    "        chain = LLMChain(llm=llm_wrapper, prompt=prompt)\n",
    "        res = chain.run(sample)\n",
    "        with chat_output:\n",
    "            print('\\n--- Resumen ---\\n')\n",
    "            print(res)\n",
    "    except Exception as e:\n",
    "        with chat_output:\n",
    "            print(f'[Error al resumir: {e}]')\n",
    "\n",
    "\n",
    "async def on_send_clicked(b):\n",
    "    global DOCUMENTS, CHUNKS\n",
    "    # construir mensajes\n",
    "    sys_msg = SystemMessage(content=system_prompt_area.value)\n",
    "    human_text = human_input.value\n",
    "    if not human_text.strip():\n",
    "        with chat_output:\n",
    "            print('Escribe tu pregunta en el campo \"Tu pregunta\"')\n",
    "        return\n",
    "    human_msg = HumanMessage(content=human_text)\n",
    "\n",
    "\n",
    "    context_tokens = context_window.value\n",
    "    context_text = ''\n",
    "    if CHUNKS:\n",
    "\n",
    "        acc = ''\n",
    "        for c in CHUNKS:\n",
    "            if len(acc) + len(c) > context_tokens:\n",
    "                break\n",
    "            acc += '\\n\\n' + c\n",
    "        context_text = acc\n",
    "\n",
    "    messages = [sys_msg]\n",
    "    if context_text:\n",
    "        messages.append(HumanMessage(content=f\"Contexto relevante:\\n{context_text}\"))\n",
    "    messages.append(human_msg)\n",
    "\n",
    "    params = {\n",
    "    'temperature': temperature_slider.value,\n",
    "    'top_p': top_p_slider.value,\n",
    "    'top_k': top_k_slider.value,\n",
    "    'max_tokens': max_tokens_int.value,\n",
    "    'model_name': 'qwen3:8b'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        with chat_output:\n",
    "            print('Enviando a LLM...')\n",
    "        llm_wrapper = await build_llm_from_selection(model_dropdown.value, params)\n",
    "    except Exception as e:\n",
    "        with chat_output:\n",
    "            print(f'[Error creando wrapper LLM: {e}]')\n",
    "        return\n",
    "\n",
    "    with chat_output:\n",
    "        print('Esperando respuesta...')\n",
    "\n",
    "    # Llamada sincrónica/asincrónica, según la implementación del wrapper\n",
    "    try:\n",
    "        if hasattr(llm_wrapper, 'generate') or hasattr(llm_wrapper, 'agenerate'):\n",
    "            # tratar de usar mensajes si el wrapper acepta\n",
    "            if hasattr(llm_wrapper, 'generate'):\n",
    "                res = llm_wrapper.generate([messages])\n",
    "                # intentar extraer texto\n",
    "                out_text = None\n",
    "                try:\n",
    "                    out_text = res.generations[0][0].text\n",
    "                except Exception:\n",
    "                    out_text = str(res)\n",
    "            else:\n",
    "                # agenerate\n",
    "                res = await llm_wrapper.agenerate(messages=[messages])\n",
    "                out_text = res.generations[0][0].text\n",
    "        else:\n",
    "            # fallback: concatenar contenido y llamar como función\n",
    "            prompt = '\\n'.join([m.content for m in messages])\n",
    "            out_text = llm_wrapper(prompt)\n",
    "    except Exception as e:\n",
    "        out_text = f'[Error llamando al LLM: {e}]'\n",
    "\n",
    "    with chat_output:\n",
    "        print('\\n--- LLM Respondió ---\\n')\n",
    "        print(out_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _sync_wrapper(coro_func):\n",
    "    def _inner(b):\n",
    "        try:\n",
    "            asyncio.create_task(coro_func(b))\n",
    "        except RuntimeError:\n",
    "            # Not in an event loop environment, run with asyncio.run\n",
    "            asyncio.run(coro_func(b))\n",
    "    return _inner\n",
    "\n",
    "wiki_button.on_click(_sync_wrapper(on_wiki_button_clicked))\n",
    "summarize_button.on_click(_sync_wrapper(on_summarize_clicked))\n",
    "send_button.on_click(_sync_wrapper(on_send_clicked))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72e901-d1a2-4277-9d61-a76c799b8ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
